---
title: "Exercise solutions: Section 3.7"
author: "Rob J Hyndman & George Athanasopoulos"
output: html_document
---
```{r, echo = F, message = F}
require(fpp2)
#setwd("C:/George/fppsolutions")
```

1. For the following series, find an appropriate Box-Cox transformation in order to stabilize the variance.

    * `usnetelec`
    * `usgdp`
    * `mcopper`
    * `enplanements`

    ```{r}
    autoplot(usnetelec)
    autoplot(usgdp)
    
    autoplot(mcopper)
    (lambda <- BoxCox.lambda(mcopper))
    mcopper %>% BoxCox(lambda=lambda) %>% autoplot()
    
    autoplot(enplanements)
    # Here we will just do a log transformation
    enplanements %>% BoxCox(lambda=0) %>% autoplot()
    ```
    The first two series don't really need a transformation. 

2. Why is a Box-Cox transformation unhelpful for the `cangas` data?

    ```{r}
    autoplot(cangas)
    (lambda <- BoxCox.lambda(cangas))
    cangas  %>% BoxCox(lambda=lambda) %>% autoplot()
    ```

    Here the variance of the series changes, but not with the level of the series. Box Cox transformations are designed to handle series where the variance increases (or decreases) with the level of the series.

3. What Box-Cox transformation would you select for your retail data (from Exercise 3 in Section \@ref(graphics-exercises))?

    ```{r, echo=TRUE, cache=TRUE}
        retaildata <- readxl::read_excel("retail.xlsx", skip=1)
        myts <- ts(retaildata[,"A3349873A"], frequency=12, start=c(1982,4))
        autoplot(myts)
        
        myts %>% BoxCox(lambda = 0) %>% autoplot()
    ```
    
    A log transformation would be appropriate here. Works well for this case and easy to explain.

4. For each of the following series, make a graph of the data. If transforming seems appropriate, do so and describe the effect. `dole`, `usdeaths`, `bricksq`.

    #### Monthly total of people on unemployed benefits in        Australia (January 1956-July 1992)

    ```{r}
    autoplot(dole)
    lambda <- BoxCox.lambda(dole)
    dole %>% BoxCox(lambda) %>%
      autoplot() + 
      ylab(paste("BoxCox(# people,", round(lambda, 2), ")"))
    ```
    
    The data was transformed using Box-Cox transformation with parameter $\lambda=`r round(lambda,2)`$. The transformation has stabilized the variance.
    
    #### Monthly total of accidental deaths in the United States (January 1973--December 1978)
    
    ```{r}
    autoplot(usdeaths) + ylab("# deaths")
    ```
    
    There is no need for a transformation for these data.
    
    #### Quarterly production of bricks (in millions of units) at Portland, Australia (March 1956--September 1994)
    
    ```{r}
    autoplot(bricksq)
    lambda <- BoxCox.lambda(bricksq)
    bricksq %>% BoxCox(lambda) %>%
      autoplot() + 
        ylab(paste("BoxCox(# mln bricks,", round(lambda, 2), ")"))
    ```
    
    The data was transformed using Box-Cox transformation with parameter $\lambda=`r round(lambda,2)`$. The transformation has stabilized the variance.
    
5. Calculate the residuals from a seasonal naïve forecast applied to the quarterly Australian beer production data from 1992. The following code will help.

    ```{r, echo=TRUE, cache=TRUE}
    beer <- window(ausbeer, start=1992)
    fc <- snaive(beer)
    autoplot(fc)
    res <- residuals(fc)
    autoplot(res)
    ```

    Test if the residuals are white noise and normally distributed.

    ```{r, echo=TRUE, cache=TRUE}
    checkresiduals(fc)
    ```

    What do you conclude?
    
    The residuals are correlated - the Null of no joint autocorrelation is clearly rejected. We can also see a singificant spike on the seasonal - 4th lag - in the ACF. There is considerable information remaining in the residuals which has not been captured with the seasonal naive method. The residuals do not appear to be too far from Normally distributed.

6. Repeat the exercise for the `WWWusage` and `bricksq` data. Use whichever of `naive` or `snaive` is more appropriate in each case.

    ```{r, echo=TRUE, cache=TRUE}
    WWWusage %>% autoplot()
    fc <- naive(WWWusage)
    res <- residuals(fc)
    autoplot(res)
    checkresiduals(fc)
    ```

    Residuals are correlated as shown by both the LB test and the ACF. They seem to be normally distributed. There is considerable information remaining in the residuals which has not been captured with the naive method.
    
    
    ```{r, echo=TRUE, cache=TRUE}
    bricksq %>% autoplot()
    fc <- snaive(bricksq)
    res <- residuals(fc)
    autoplot(res)
    checkresiduals(fc)
    ```

    Residuals are correlated as shown by both the LB test and the ACF and do not appear to be normal (they have a long left tail). There is considerable information remaining in the residuals which has not been captured with the seasonal naive method.

7. Are the following statements true or false? Explain your answer.

    a. Good forecast methods should have normally distributed residuals.
    b. A model with small residuals will give good forecasts.
    c. The best measure of forecast accuracy is MAPE.
    d. If your model doesn't forecast well, you should make it more complicated.
    e. Always choose the model with the best forecast accuracy as measured on the test set.


8. For your retail time series (from Exercise 3 in Section \@ref(graphics-exercises)):

    a. Split the data into two parts using

    ```{r, echo=TRUE, cache=TRUE}
    retaildata <- readxl::read_excel("retail.xlsx", skip=1)
        myts <- ts(retaildata[,"A3349873A"], frequency=12, start=c(1982,4))
    myts.train <- window(myts, end=c(2010,12))
    myts.test <- window(myts, start=2011)
    ```

    b. Check that your data have been split appropriately by producing the following plot.

    ```{r, echo=TRUE, cache=TRUE}
    autoplot(myts) + 
      autolayer(myts.train, series="Training") +
      autolayer(myts.test, series="Test")
    ```

    c. Calculate forecasts using `snaive` applied to `myts.train`.

    ```{r, echo=TRUE, cache=TRUE}
    fc <- snaive(myts.train)
    ```

    d. Compare the accuracy of your forecasts against the actual values stored in `myts.test`.

    ```{r, echo=TRUE, cache=TRUE}
      accuracy(fc,myts.test)
    ```

    e. Check the residuals. Do the residuals appear to be uncorrelated and normally distributed?

    ```{r, echo=TRUE, cache=TRUE}
    checkresiduals(fc)
    ```

    The resituals do not look like white noise there are lots of dynamics left over that need to be explored. They do look close to normal although the tails may be too long.

    f. How sensitive are the accuracy measures to the training/test split?

    The accuracy measure are always sensitive to this split. There are better ways to check the robustness of the methods in terms of accuracy such as using a rolling window (possible in this case as we have lots of data) or ts.cv. 

9. `vn` contains quarterly visitor nights (in millions) from 1998-2015 for eight regions of Australia. 

    a. Use `window()` to create three training sets for `vn[,"Melbourne"],` omitting the last 1, 2 and 3 years; call these train1, train2, and train3, respectively. For example `train1 <- window(vn[, "Melbourne"], end = c(2014, 4))`. 
    
    b. Compute one year of forecasts for each training set using the `snaive()` method. Call these `fc1`, `fc2` and `fc3`, respectively.

    c. Use `accuracy()` to compare the MAPE over the three test sets. Comment on these.


10. Use the Dow Jones index (data set `dowjones`) to do the following:

    a.  Produce a time plot of the series.
    b.  Produce forecasts using the drift method and plot them.
    c.  Show that the forecasts are identical to extending the line drawn between the first and last observations.
    d.  Try using some of the other benchmark functions to forecast the same data set. Which do you think is best? Why?

11. Consider the daily closing IBM stock prices (data set `ibmclose`).

    a. Produce some plots of the data in order to become familiar with it.
    ```{r, echo=TRUE, cache=TRUE}
    autoplot(ibmclose) + ylab("Value $")
    ```
    
    b.  Split the data into a training set of 300 observations and a test set of 69 observations.
    ```{r, echo=TRUE, cache=TRUE}
    ibm.train <- window(ibmclose, end=300)
    ibm.test <- window(ibmclose, start=301)
    autoplot(ibmclose) + 
      autolayer(ibm.train, series="Training") +
      autolayer(ibm.test, series="Test")
    ```

    c.  Try using various benchmark methods to forecast the training set and compare the results on the test set. Which method did best?
  
    ```{r, echo=TRUE, cache=TRUE}
    h <- length(ibm.test)
    m.f <- meanf(ibm.train, h=h)
    rw.f <- rwf(ibm.train, h=h)
    rwd.f <- rwf(ibm.train, drift=TRUE, h=h)

    autoplot(ibmclose) +
      xlab("Day") +
      ggtitle("Daily closing IBM stock prices") +
      autolayer(m.f$mean, col=2, series="Mean method") +
      autolayer(rw.f$mean, col=3, series="Naive method") +
      autolayer(rwd.f$mean, col=4, series="Drift method")

    accuracy(m.f,ibm.test)
    accuracy(rw.f,ibm.test)
    accuracy(rwd.f,ibm.test)

    ```
    
    In terms of accuracy measures on the test set, the drift method does better.

    d. Check the residuals of your preferred method. Do they resemble white noise?

    ```{r, echo=TRUE, cache=TRUE}
    checkresiduals(rwd.f)
    Box.test(residuals(rwd.f), type="Lj",lag = 10, fitdf=1)

    ```
      
      Residuals look relatively uncorrelated, but the distribution is not normal (tails too long).

12.  Consider the sales of new one-family houses in the USA, Jan 1973 -- Nov 1995 (data set `hsales`).

    a. Produce some plots of the data in order to become familiar with it.
    
    ```{r, echo=TRUE, cache=TRUE}
    autoplot(hsales) + ylab("Sales")
    ```

    b.  Split the `hsales` data set into a training set and a test set, where the test set is the last two years of data.
    ```{r, echo=TRUE, cache=TRUE}
    hsales.train = window(hsales, end=c(1993,11))
    hsales.test = window(hsales, start=c(1993,12))

    autoplot(hsales) + 
      autolayer(hsales.train, series="Training") +
      autolayer(hsales.test, series="Test")

    ```

    c.  Try using various benchmark methods to forecast the training set and compare the results on the test set. Which method did best?
    ```{r, echo=TRUE, cache=TRUE}

    h <- length(hsales.test)
    m.f <- meanf(hsales.train, h=h)
    rw.f <- rwf(hsales.train, h=h)
    sn.f <- snaive(hsales.train, h=h)
    rwd.f <- rwf(hsales.train, drift=TRUE, h=h)

    autoplot(hsales) +
      xlab("Year") +
      ggtitle("Sales") +
      autolayer(m.f$mean, col=2, series="Mean method") +
      autolayer(rw.f$mean, col=3, series="Naive method") +
      autolayer(sn.f$mean, col=4, series="Seasonal naive method") +
      autolayer(rwd.f$mean, col=5, series="Drift method")

    accuracy(m.f,hsales.test)
    accuracy(rw.f,hsales.test)
    accuracy(sn.f,hsales.test)
    accuracy(rwd.f,hsales.test)
    ```

    In terms of accuracy measures on the test set, the seasonal naive method does better.

    d. Check the residuals of your preferred method. Do they resemble white noise?
    ```{r, echo=TRUE, cache=TRUE}
    checkresiduals(sn.f)    
    ```

    Residuals are correlated. They show some cyclic behaviour. They seem normal.





